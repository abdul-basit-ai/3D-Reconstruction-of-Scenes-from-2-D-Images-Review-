# 3D-Reconstruction-of-Scenes-from-2-D-Images-Review-
Abstractâ€” This paper presents a comprehensive review of recent advances in 3D scene reconstruction using only 2D images.While traditional approaches predominantly rely on
geometric principles, such as multi-view stereo and structurefrom-motion, they often require dense image inputs and struggle
with occlusions, textureless regions, and viewpoint sparsity.
Modern techniques, driven by deep learning, offer a paradigm
shift by directly learning 3D structure from 2D observations
through optimization or generative modeling. Among these,
Neural Radiance Fields (NeRF), Gaussian Splatting, and
SparseFusion have emerged as prominent frameworks, each
introducing unique mechanisms for representing, rendering,
and learning 3D scenes. These methods leverage neural
networks, volumetric rendering, and generative priors to
achieve high-fidelity reconstructions from minimal input data.
This review highlights their architectural foundations, learning
strategies, and limitations, providing a consolidated
understanding of the current state of the art. The paper aims to
serve as a foundation for future research in neural 3D
reconstruction and its applications in computer vision, graphics,
robotics, and immersive technologies 
